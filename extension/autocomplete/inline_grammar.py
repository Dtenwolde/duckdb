import argparse
import os
import sys
from enum import Enum, auto
from pathlib import Path

parser = argparse.ArgumentParser(
    description="Inline the auto-complete PEG grammar files"
)
parser.add_argument(
    "--print",
    action="store_true",
    help="Print the grammar instead of writing to a file",
    default=False,
)
parser.add_argument(
    "--grammar-file",
    action="store_true",
    help="Write the grammar to a .gram file instead of a C++ header",
    default=False,
)

args = parser.parse_args()

autocomplete_dir = Path(__file__).parent
statements_dir = os.path.join(autocomplete_dir, "grammar", "statements")
keywords_dir = os.path.join(autocomplete_dir, "grammar", "keywords")
target_file = os.path.join(autocomplete_dir, "include", "inlined_grammar.hpp")

contents = ""

# Maps filenames to string categories
FILENAME_TO_CATEGORY = {
    "reserved_keyword.list": "RESERVED_KEYWORD",
    "unreserved_keyword.list": "UNRESERVED_KEYWORD",
    "column_name_keyword.list": "COL_NAME_KEYWORD",
    "func_name_keyword.list": "TYPE_FUNC_NAME_KEYWORD",
    "type_name_keyword.list": "TYPE_FUNC_NAME_KEYWORD",
}

# Maps category names to their C++ map variable names
CPP_MAP_NAMES = {
    "RESERVED_KEYWORD": "reserved_keyword_map",
    "UNRESERVED_KEYWORD": "unreserved_keyword_map",
    "COL_NAME_KEYWORD": "colname_keyword_map",
    "TYPE_FUNC_NAME_KEYWORD": "typefunc_keyword_map",
}

# Use a dictionary of sets to collect keywords for each category, preventing duplicates
keyword_sets = {category: set() for category in CPP_MAP_NAMES.keys()}

# --- Validation and Loading (largely unchanged) ---
# For validation during the loading phase
reserved_set = set()
unreserved_set = set()


def load_keywords(filepath):
    with open(filepath, "r") as f:
        return [line.strip().lower() for line in f if line.strip()]


for filename in os.listdir(keywords_dir):
    if filename not in FILENAME_TO_CATEGORY:
        continue

    category = FILENAME_TO_CATEGORY[filename]
    keywords = load_keywords(os.path.join(keywords_dir, filename))

    for kw in keywords:
        # Validation logic remains the same to enforce rules
        if category == "RESERVED_KEYWORD":
            if kw in reserved_set or kw in unreserved_set:
                print(f"Keyword '{kw}' has conflicting RESERVED/UNRESERVED categories")
                exit(1)
            reserved_set.add(kw)
        elif category == "UNRESERVED_KEYWORD":
            if kw in reserved_set or kw in unreserved_set:
                print(f"Keyword '{kw}' has conflicting RESERVED/UNRESERVED categories")
                exit(1)
            unreserved_set.add(kw)

        # Add the keyword to the appropriate set
        keyword_sets[category].add(kw)

# --- C++ Code Generation ---
output_path = os.path.join(autocomplete_dir, "keyword_map.cpp")
with open(output_path, "w") as f:
    f.write("/* THIS FILE WAS AUTOMATICALLY GENERATED BY inline_grammar.py */\n")
    f.write('#include "keyword_helper.hpp"\n\n')
    f.write("namespace duckdb {\n")
    f.write("void PEGKeywordHelper::InitializeKeywordMaps() { // Renamed for clarity\n")
    f.write("\tif (initialized) {\n\t\treturn;\n\t};\n")
    f.write("\tinitialized = true;\n\n")

    # Get the total number of categories to handle the last item differently
    num_categories = len(keyword_sets)

    # Iterate through each category and generate code for each map
    for i, (category, keywords) in enumerate(keyword_sets.items()):
        cpp_map_name = CPP_MAP_NAMES[category]
        f.write(f"\t// Populating {cpp_map_name}\n")
        # Sort keywords for deterministic output
        for kw in sorted(list(keywords)):
            # Populate the C++ set with insert
            f.write(f'\t{cpp_map_name}.insert("{kw}");\n')

        # Add a newline for all but the last block
        if i < num_categories - 1:
            f.write("\n")
    f.write("}\n")
    f.write("} // namespace duckdb\n")


def filename_to_upper_camel(file):
    name, _ = os.path.splitext(file)  # column_name_keywords
    parts = name.split("_")  # ['column', 'name', 'keywords']
    return "".join(p.capitalize() for p in parts)


# --- PEG Grammar Validation ---
# Built-in rule overrides registered in MatcherFactory::CreateMatcher() (matcher.cpp)
RULE_OVERRIDES = {
    "Identifier",
    "ReservedIdentifier",
    "CatalogName",
    "SchemaName",
    "ReservedSchemaName",
    "TableName",
    "ReservedTableName",
    "ColumnName",
    "ReservedColumnName",
    "IndexName",
    "SequenceName",
    "FunctionName",
    "ReservedFunctionName",
    "TableFunctionName",
    "TypeName",
    "PragmaName",
    "SettingName",
    "CopyOptionName",
    "NumberLiteral",
    "StringLiteral",
    "OperatorLiteral",
}

PEG_OPERATORS = set("/?*()+!")


class PEGTokenType(Enum):
    LITERAL = auto()
    REFERENCE = auto()
    OPERATOR = auto()
    FUNCTION_CALL = auto()
    REGEX = auto()


class PEGParseState(Enum):
    RULE_NAME = auto()
    RULE_SEPARATOR = auto()
    RULE_DEFINITION = auto()


def pos_to_file_line(pos, file_offsets, grammar_text):
    """Map a character position in the concatenated grammar to (filename, line_number)."""
    filename = file_offsets[0][1]
    file_start = 0
    for offset, name in file_offsets:
        if offset > pos:
            break
        filename = name
        file_start = offset
    line = grammar_text[file_start:pos].count("\n") + 1
    return filename, line


def parse_peg_grammar(grammar_text, file_offsets):
    """Parse PEG grammar text into a dict of rule_name -> (tokens, parameters, start_pos).
    Mirrors PEGParser::ParseRules() from peg_parser.cpp.
    Returns dict mapping rule_name to {'tokens': [...], 'parameters': [...], 'start_pos': int}.
    Raises RuntimeError on syntax errors.
    """
    grammar = grammar_text
    rules = {}
    rule_name = None
    rule_start_pos = 0
    tokens = []
    parameters = []
    state = PEGParseState.RULE_NAME
    bracket_count = 0
    in_or_clause = False
    c = 0
    length = len(grammar)

    def loc(pos):
        filename, line = pos_to_file_line(pos, file_offsets, grammar)
        return f"{filename}:{line}"

    while c < length:
        ch = grammar[c]

        # Comments
        if ch == "#":
            while c < length and grammar[c] != "\n":
                c += 1
            continue

        # Newline ends a rule definition if brackets are balanced and not in an or clause
        if (
            state == PEGParseState.RULE_DEFINITION
            and ch == "\n"
            and bracket_count == 0
            and not in_or_clause
            and len(tokens) > 0
        ):
            if rule_name in rules:
                raise RuntimeError(
                    f"{loc(rule_start_pos)}: Duplicate rule name '{rule_name}'"
                )
            rules[rule_name] = {
                "tokens": list(tokens),
                "parameters": list(parameters),
                "start_pos": rule_start_pos,
            }
            rule_name = None
            tokens = []
            parameters = []
            state = PEGParseState.RULE_NAME
            c += 1
            continue

        # Skip whitespace
        if ch in " \t\r\n":
            c += 1
            continue

        if state == PEGParseState.RULE_NAME:
            start = c
            if ch == "%":
                c += 1
            while c < length and grammar[c].isalnum():
                c += 1
            if c == start:
                raise RuntimeError(f"{loc(c)}: Expected alphanumeric rule name")
            rule_name = grammar[start:c]
            rule_start_pos = start
            tokens = []
            parameters = []
            state = PEGParseState.RULE_SEPARATOR

        elif state == PEGParseState.RULE_SEPARATOR:
            if ch == "(":
                # Parse parameter
                if parameters:
                    raise RuntimeError(
                        f"{loc(c)}: Multiple parameter lists in rule '{rule_name}'"
                    )
                c += 1
                param_start = c
                while c < length and grammar[c].isalnum():
                    c += 1
                if c == param_start:
                    raise RuntimeError(
                        f"{loc(c)}: Expected parameter name in rule '{rule_name}'"
                    )
                parameters.append(grammar[param_start:c])
                if c >= length or grammar[c] != ")":
                    raise RuntimeError(
                        f"{loc(c)}: Expected closing ')' for parameter in rule '{rule_name}'"
                    )
                c += 1
            else:
                if c + 1 >= length or grammar[c] != "<" or grammar[c + 1] != "-":
                    raise RuntimeError(
                        f"{loc(c)}: Expected '<-' after rule name '{rule_name}'"
                    )
                c += 2
                state = PEGParseState.RULE_DEFINITION

        elif state == PEGParseState.RULE_DEFINITION:
            in_or_clause = False
            if ch == "'":
                # Literal
                c += 1
                lit_start = c
                while c < length and grammar[c] != "'":
                    if grammar[c] == "\\":
                        c += 1
                    c += 1
                if c >= length:
                    raise RuntimeError(
                        f"{loc(lit_start - 1)}: Unclosed quote in rule '{rule_name}'"
                    )
                tokens.append((PEGTokenType.LITERAL, grammar[lit_start:c]))
                c += 1
                if c < length and grammar[c] == "i":
                    raise RuntimeError(
                        f"{loc(c)}: Unexpected 'i' suffix on literal in rule '{rule_name}'"
                    )
            elif ch.isalnum():
                # Reference or function call
                ref_start = c
                while c < length and grammar[c].isalnum():
                    c += 1
                ref_text = grammar[ref_start:c]
                if c < length and grammar[c] == "(":
                    c += 1
                    bracket_count += 1
                    tokens.append((PEGTokenType.FUNCTION_CALL, ref_text))
                else:
                    tokens.append((PEGTokenType.REFERENCE, ref_text))
            elif ch == "[" or ch == "<":
                # Regex
                regex_start = c
                close = "]" if ch == "[" else ">"
                c += 1
                while c < length and grammar[c] != close:
                    if grammar[c] == "\\":
                        c += 1
                    if c < length:
                        c += 1
                if c >= length:
                    raise RuntimeError(
                        f"{loc(regex_start)}: Unclosed '{ch}' in rule '{rule_name}'"
                    )
                c += 1
                tokens.append((PEGTokenType.REGEX, grammar[regex_start:c]))
            elif ch in PEG_OPERATORS:
                if ch == "(":
                    bracket_count += 1
                elif ch == ")":
                    if bracket_count == 0:
                        raise RuntimeError(
                            f"{loc(c)}: Unbalanced ')' in rule '{rule_name}'"
                        )
                    bracket_count -= 1
                elif ch == "/":
                    in_or_clause = True
                tokens.append((PEGTokenType.OPERATOR, ch))
                c += 1
            else:
                raise RuntimeError(
                    f"{loc(c)}: Unrecognized character '{ch}' in rule '{rule_name}'"
                )

        if c >= length:
            break

    # Handle final rule
    if state == PEGParseState.RULE_SEPARATOR:
        raise RuntimeError(
            f"{loc(rule_start_pos)}: Rule '{rule_name}' does not have a definition"
        )
    if state == PEGParseState.RULE_DEFINITION:
        if not tokens:
            raise RuntimeError(f"{loc(rule_start_pos)}: Rule '{rule_name}' is empty")
        if rule_name in rules:
            raise RuntimeError(
                f"{loc(rule_start_pos)}: Duplicate rule name '{rule_name}'"
            )
        rules[rule_name] = {
            "tokens": list(tokens),
            "parameters": list(parameters),
            "start_pos": rule_start_pos,
        }

    return rules


def validate_references(rules, rule_locations):
    """Check that every REFERENCE and FUNCTION_CALL points to a defined rule, override, or parameter."""
    errors = []
    for rule_name, rule in rules.items():
        param_names = set(rule["parameters"])
        loc = rule_locations[rule_name]
        for token_type, token_text in rule["tokens"]:
            if token_type in (PEGTokenType.REFERENCE, PEGTokenType.FUNCTION_CALL):
                if (
                    token_text not in rules
                    and token_text not in RULE_OVERRIDES
                    and token_text not in param_names
                ):
                    kind = (
                        "calls"
                        if token_type == PEGTokenType.FUNCTION_CALL
                        else "references"
                    )
                    errors.append(
                        f"{loc}: Rule '{rule_name}' {kind} undefined rule '{token_text}'"
                    )
    return errors


def find_unused_rules(rules, rule_locations):
    """Walk from 'Statement' and report rules not reachable from the root."""
    if "Statement" not in rules:
        return []  # Can't check without root

    visited = set()
    stack = ["Statement"]
    while stack:
        name = stack.pop()
        if name in visited:
            continue
        visited.add(name)
        if name not in rules:
            continue
        for token_type, token_text in rules[name]["tokens"]:
            if token_type in (PEGTokenType.REFERENCE, PEGTokenType.FUNCTION_CALL):
                if token_text in rules and token_text not in visited:
                    stack.append(token_text)

    warnings = []
    for rule_name in sorted(rules.keys()):
        if (
            rule_name not in visited
            and rule_name not in RULE_OVERRIDES
            and rule_name != "%whitespace"
        ):
            loc = rule_locations[rule_name]
            warnings.append(
                f"{loc}: Rule '{rule_name}' is defined but not reachable from 'Statement'"
            )
    return warnings


def validate_grammar(grammar_text, file_offsets):
    """Run all grammar validation phases. Exits on fatal errors."""
    # Phase 1: Parse
    try:
        rules = parse_peg_grammar(grammar_text, file_offsets)
    except RuntimeError as e:
        print(f"Grammar syntax error: {e}", file=sys.stderr)
        sys.exit(1)

    # Build location map: rule_name -> "file:line"
    rule_locations = {}
    for name, rule in rules.items():
        filename, line = pos_to_file_line(rule["start_pos"], file_offsets, grammar_text)
        rule_locations[name] = f"{filename}:{line}"

    # Phase 2: Validate references
    ref_errors = validate_references(rules, rule_locations)
    if ref_errors:
        for err in ref_errors:
            print(f"Grammar reference error: {err}", file=sys.stderr)
        sys.exit(1)

    # Phase 3: Unused rules (warnings only)
    unused_warnings = find_unused_rules(rules, rule_locations)
    for warn in unused_warnings:
        print(f"Grammar warning: {warn}", file=sys.stderr)

    print(f"Grammar validation passed ({len(rules)} rules)")


file_offsets = []

file_offsets.append((len(contents), "common.gram"))
with open(os.path.join(statements_dir, "common.gram"), "r") as f:
    contents += f.read() + "\n"

for file in os.listdir(keywords_dir):
    if not file.endswith(".list"):
        continue
    rule_name = filename_to_upper_camel(file)
    file_offsets.append((len(contents), file))
    rule = f"{rule_name} <- "
    with open(os.path.join(keywords_dir, file), "r") as f:
        lines = [f"'{line.strip()}'" for line in f if line.strip()]
        rule += " /\n".join(lines) + "\n"
    contents += rule

for file in os.listdir(statements_dir):
    if not file.endswith(".gram"):
        raise Exception(f"File {file} does not end with .gram")
    if not file == "common.gram":
        file_offsets.append((len(contents), file))
        with open(os.path.join(statements_dir, file), "r") as f:
            contents += f.read() + "\n"

validate_grammar(contents, file_offsets)

if args.print:
    print(contents)
    exit(0)

if args.grammar_file:
    grammar_file = target_file.replace(".hpp", ".gram")
    with open(grammar_file, "w+") as f:
        f.write(contents)
    print(f"Successfully generated {output_path} and {grammar_file}")
    exit(0)


def get_grammar_bytes(contents, add_null_terminator=True):
    result_text = ""
    for line in contents.split("\n"):
        if len(line) == 0:
            continue
        result_text += '\t"' + line.replace("\\", "\\\\").replace('"', '\\"') + '\\n"\n'
    return result_text


with open(target_file, "w+") as f:
    f.write(
        """/* THIS FILE WAS AUTOMATICALLY GENERATED BY inline_grammar.py */
#pragma once

namespace duckdb {

const char INLINED_PEG_GRAMMAR[] = {
"""
        + get_grammar_bytes(contents)
        + """
};

} // namespace duckdb
"""
    )

print(f"Successfully generated {output_path} and {target_file}")
